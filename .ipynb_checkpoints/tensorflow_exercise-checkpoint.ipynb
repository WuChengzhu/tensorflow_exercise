{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用 NumPy 生成假数据(phony data), 总共 100 个点.\n",
    "x_data = np.float32(np.random.rand(2, 100)) # 随机输入\n",
    "y_data = np.dot([0.100, 0.200], x_data) + 0.300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造一个线性模型\n",
    "# \n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))\n",
    "y = tf.matmul(W, x_data) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[-0.5526787,  1.027071 ]], dtype=float32), array([ 0.42350525], dtype=float32)]\n",
      "20 [array([[-0.03690662,  0.3674413 ]], dtype=float32), array([ 0.28259146], dtype=float32)]\n",
      "40 [array([[ 0.07512739,  0.23839875]], dtype=float32), array([ 0.29263803], dtype=float32)]\n",
      "60 [array([[ 0.09576317,  0.20909017]], dtype=float32), array([ 0.29740426], dtype=float32)]\n",
      "80 [array([[ 0.09936837,  0.2022239 ]], dtype=float32), array([ 0.2991558], dtype=float32)]\n",
      "100 [array([[ 0.09993653,  0.20056173]], dtype=float32), array([ 0.29973716], dtype=float32)]\n",
      "120 [array([[ 0.10000557,  0.20014605]], dtype=float32), array([ 0.29992023], dtype=float32)]\n",
      "140 [array([[ 0.10000642,  0.20003897]], dtype=float32), array([ 0.29997617], dtype=float32)]\n",
      "160 [array([[ 0.10000281,  0.20001058]], dtype=float32), array([ 0.29999298], dtype=float32)]\n",
      "180 [array([[ 0.10000101,  0.20000292]], dtype=float32), array([ 0.29999793], dtype=float32)]\n",
      "200 [array([[ 0.10000034,  0.20000082]], dtype=float32), array([ 0.29999939], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# 最小化方差\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 启动图 (graph)，创建一个会话 \n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "#tf.global_variables_initializer().run(session=sess)\n",
    "# 拟合平面\n",
    "for step in range(0, 201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run([W, b]))\n",
    "#关闭当前会话\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常量计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常量计算与其他语言类似，但是如同Spark里面弹性(resilient)变量一样，只有在会话里运行之后，各种运算才开始进行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建一个常量 op, 产生一个 1x2 矩阵. 这个 op 被作为一个节点\n",
    "# 加到默认图中.\n",
    "#\n",
    "# 构造器的返回值代表该常量 op 的返回值.\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "\n",
    "# 创建另外一个常量 op, 产生一个 2x1 矩阵.\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "# 创建一个矩阵乘法 matmul op , 把 'matrix1' 和 'matrix2' 作为输入.\n",
    "# 返回值 'product' 代表矩阵乘法的结果.\n",
    "product = tf.matmul(matrix1, matrix2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果不运行会话，矩阵里面没有内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_23:0\", shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_1:0\", shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "# 启动默认图.\n",
    "sess = tf.Session()\n",
    "\n",
    "# 调用 sess 的 'run()' 方法来执行矩阵乘法 op, 传入 'product' 作为该方法的参数. \n",
    "# 上面提到, 'product' 代表了矩阵乘法 op 的输出, 传入它是向方法表明, 我们希望取回\n",
    "# 矩阵乘法 op 的输出.\n",
    "#\n",
    "# 整个执行过程是自动化的, 会话负责传递 op 所需的全部输入. op 通常是并发执行的.\n",
    "# \n",
    "# 函数调用 'run(product)' 触发了图中三个 op (两个常量 op 和一个矩阵乘法 op) 的执行.\n",
    "#\n",
    "# 返回值 'result' 是一个 numpy `ndarray` 对象.\n",
    "result = sess.run(product)\n",
    "print(result)\n",
    "# ==> [[ 12.]]\n",
    "\n",
    "# 任务完成, 关闭会话.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "#启动会话运行之后才能得到结果\n",
    "print(product.eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  3.]]\n"
     ]
    }
   ],
   "source": [
    "print(matrix1.eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  3.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(matrix1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交互式会话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2. -1.]\n"
     ]
    }
   ],
   "source": [
    "# 进入一个交互式 TensorFlow 会话.\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.Variable([1.0, 2.0])\n",
    "a = tf.constant([3.0, 3.0])\n",
    "\n",
    "# 使用初始化器 initializer op 的 run() 方法初始化 'x' \n",
    "x.initializer.run()\n",
    "\n",
    "# 增加一个减法 sub op, 从 'x' 减去 'a'. 运行减法 op, 输出结果 \n",
    "sub = tf.sub(x, a)\n",
    "print(sub.eval())\n",
    "# ==> [-2. -1.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 创建一个变量, 初始化为标量 0.\n",
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "# 创建一个 op, 其作用是使 state 增加 1\n",
    "\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "# 启动图后, 变量必须先经过`初始化` (init) op 初始化,\n",
    "# 首先必须增加一个`初始化` op 到图中.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# 启动图, 运行 op\n",
    "with tf.Session() as sess:\n",
    "  # 运行 'init' op\n",
    "  sess.run(init_op)\n",
    "  # 打印 'state' 的初始值\n",
    "  print(sess.run(state))\n",
    "  # 运行 op, 更新 'state', 并打印 'state'\n",
    "  for _ in range(3):\n",
    "    sess.run(update)\n",
    "    print(sess.run(state))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过feed机制可以将Python数据直接传递给tensorflow。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 14.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output = tf.mul(input1, input2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面例子是通过简单单层神经网络对手写数字进行分类识别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#读取数据\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = mnist.train.images\n",
    "train_labels = mnist.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#定义feed数据\n",
    "#定义输入数据\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "#输入数据对应标签\n",
    "y_ = tf.placeholder(tf.float32, [None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#定义权重和偏置参数变量\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#系统输出标签值 \n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义目标函数，采用交叉熵\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义训练方式，梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试精度： 0.3045\n",
      "测试精度： 0.8699\n",
      "测试精度： 0.8743\n",
      "测试精度： 0.8549\n",
      "测试精度： 0.9014\n",
      "测试精度： 0.8833\n",
      "测试精度： 0.9072\n",
      "测试精度： 0.9033\n",
      "测试精度： 0.9099\n",
      "测试精度： 0.9107\n",
      "测试精度： 0.9028\n",
      "测试精度： 0.9096\n",
      "测试精度： 0.9097\n",
      "测试精度： 0.8926\n",
      "测试精度： 0.9151\n",
      "测试精度： 0.9106\n",
      "测试精度： 0.9122\n",
      "测试精度： 0.8925\n",
      "测试精度： 0.9165\n",
      "测试精度： 0.9188\n",
      "测试精度： 0.8992\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "batch_size = 64\n",
    "#创建会话\n",
    "with tf.Session() as sess:\n",
    "    #初始化所有变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        #采用随机梯队下降方法，每次选部分样本进行训练      \n",
    "        batch_data = train_images[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        #将数据传递给字典\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "        _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "        if step%50 == 0:\n",
    "            #定义模型评价指标精确度\n",
    "            correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            result = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            print(\"测试精度：\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST进阶（卷积神经网络）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建权重变量\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "#创建偏差\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#卷积函数\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "#池化函数\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#变量定义\n",
    "#定义输入数据\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "#期望输出标签\n",
    "y_ = tf.placeholder(tf.float32, [None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第一层卷积\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "#卷积后采用Relu函数激活\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "#进行池化\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第二层卷积\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "#卷积后采用Relu函数激活\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "#进行池化\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#全连接层\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#丢弃，为了增强泛化能力\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#输出层，softmax\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#损失函数\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "#梯度下降法\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "#计算准确度\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.1139\n",
      "Accuracy 0.7469\n",
      "Accuracy 0.8609\n",
      "Accuracy 0.8959\n",
      "Accuracy 0.9129\n",
      "Accuracy 0.9256\n",
      "Accuracy 0.9374\n",
      "Accuracy 0.9373\n",
      "Accuracy 0.945\n",
      "Accuracy 0.9445\n",
      "Accuracy 0.9461\n",
      "Accuracy 0.9526\n",
      "Accuracy 0.9536\n",
      "Accuracy 0.9566\n",
      "Accuracy 0.9582\n",
      "Accuracy 0.9581\n",
      "Accuracy 0.9606\n",
      "Accuracy 0.9592\n",
      "Accuracy 0.9633\n",
      "Accuracy 0.9637\n",
      "Accuracy 0.9651\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "batch_size = 64\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        #产生训练用样本集      \n",
    "        batch_data = train_images[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        #数据传递给tensorflow\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels, keep_prob:0.5}\n",
    "        sess.run(train_step, feed_dict=feed_dict)\n",
    "        if step%50 == 0:\n",
    "            #每50次计算准确度\n",
    "            result = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob:1})\n",
    "            print('Accuracy', result)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 命名空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#手写数字分为10各类，即0-9\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "#图片像素28*28\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "\n",
    "def inference(images, hidden1_units, hidden2_units):\n",
    "  \"\"\"建立双层MNIST神经网络模型\n",
    "\n",
    "  Args:\n",
    "    images: 输入的像素值\n",
    "    hidden1_units: 第一隐层神经元数量\n",
    "    hidden2_units: 第二隐层神经元数量\n",
    "\n",
    "  Returns:\n",
    "    softmax_linear: 归一化后的输出结果\n",
    "  \"\"\"\n",
    "  # 隐层1\n",
    "  with tf.name_scope('hidden1'):\n",
    "    #权重\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([IMAGE_PIXELS, hidden1_units],\n",
    "                            stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "        name='weights')\n",
    "    #偏差\n",
    "    biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                         name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "  # 隐层2\n",
    "  with tf.name_scope('hidden2'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                         name='biases')\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "  # Linear\n",
    "  with tf.name_scope('softmax_linear'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([hidden2_units, NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([NUM_CLASSES]),\n",
    "                         name='biases')\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "  return logits\n",
    "\n",
    "\n",
    "def loss(logits, labels):\n",
    "  \"\"\"计算损失值\n",
    "\n",
    "  Args:\n",
    "    logits: 预测值，张量, float - [batch_size, NUM_CLASSES].\n",
    "    labels: 原始标签值，张量, int32 - [batch_size].\n",
    "\n",
    "  Returns:\n",
    "    loss: 损失函数 float.\n",
    "  \"\"\"\n",
    "  #labels = tf.to_int64(labels)\n",
    "  #cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      #logits, labels, name='xentropy')\n",
    "  #loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "  #交叉熵\n",
    "  y_ = tf.nn.softmax(logits, name='xentropy')\n",
    "  loss = -tf.reduce_mean(labels*tf.log(y_), name='xentropy_mean')\n",
    "  return loss\n",
    "\n",
    "\n",
    "def training(loss, learning_rate):\n",
    "  \"\"\"建立训练过程\n",
    "\n",
    "  创建summarizer以在tensorboard里跟踪损失值\n",
    "\n",
    "  创建优化器，采用梯度下降方法训练模型\n",
    "\n",
    "\n",
    "  参数:\n",
    "    loss: 损失值.\n",
    "    learning_rate: 学习速率.\n",
    "\n",
    "  返回:\n",
    "    train_op: 训练设置.\n",
    "  \"\"\"\n",
    "  # Add a scalar summary for the snapshot loss.\n",
    "  tf.summary.scalar(loss.op.name, loss)\n",
    "  # Create the gradient descent optimizer with the given learning rate.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  # Create a variable to track the global step.\n",
    "  global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "  # Use the optimizer to apply the gradients that minimize the loss\n",
    "  # (and also increment the global step counter) as a single training step.\n",
    "  train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "  return train_op\n",
    "\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "  \"\"\"评估模型预测质量.\n",
    "\n",
    "  参数:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size], with values in the\n",
    "      range [0, NUM_CLASSES).\n",
    "\n",
    "  返回:\n",
    "    A scalar int32 tensor with the number of examples (out of batch_size)\n",
    "    that were predicted correctly.\n",
    "  \"\"\"\n",
    "    #计算准确度\n",
    "  y_hat = tf.nn.softmax(logits, name='xentropy')\n",
    "  correct_prediction = tf.equal(tf.argmax(y_hat,1), tf.argmax(labels,1))\n",
    "\n",
    "  return tf.reduce_sum(tf.cast(correct_prediction, tf.int32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size):\n",
    "  \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "\n",
    "  These placeholders are used as inputs by the rest of the model building\n",
    "  code and will be fed from the downloaded data in the .run() loop, below.\n",
    "\n",
    "  Args:\n",
    "    batch_size: The batch size will be baked into both placeholders.\n",
    "\n",
    "  Returns:\n",
    "    images_placeholder: Images placeholder.\n",
    "    labels_placeholder: Labels placeholder.\n",
    "  \"\"\"\n",
    "  # Note that the shapes of the placeholders match the shapes of the full\n",
    "  # image and label tensors, except the first dimension is now batch_size\n",
    "  # rather than the full size of the train or test data sets.\n",
    "  images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                         IMAGE_PIXELS))\n",
    "  labels_placeholder = tf.placeholder(tf.float32, shape=(batch_size, NUM_CLASSES))\n",
    "  return images_placeholder, labels_placeholder\n",
    "\n",
    "\n",
    "def fill_feed_dict(data_set, batch_size, images_pl, labels_pl):\n",
    "  \"\"\"Fills the feed_dict for training the given step.\n",
    "\n",
    "  A feed_dict takes the form of:\n",
    "  feed_dict = {\n",
    "      <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "      ....\n",
    "  }\n",
    "\n",
    "  Args:\n",
    "    data_set: The set of images and labels, from input_data.read_data_sets()\n",
    "    images_pl: The images placeholder, from placeholder_inputs().\n",
    "    labels_pl: The labels placeholder, from placeholder_inputs().\n",
    "\n",
    "  Returns:\n",
    "    feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "  \"\"\"\n",
    "  # Create the feed_dict for the placeholders filled with the next\n",
    "  # `batch size` examples.\n",
    "  images_feed, labels_feed = data_set.next_batch(batch_size)\n",
    "  feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "  }\n",
    "  return feed_dict\n",
    "\n",
    "\n",
    "def do_eval(sess, batch_size,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "  \"\"\"Runs one evaluation against the full epoch of data.\n",
    "\n",
    "  Args:\n",
    "    sess: The session in which the model has been trained.\n",
    "    eval_correct: The Tensor that returns the number of correct predictions.\n",
    "    images_placeholder: The images placeholder.\n",
    "    labels_placeholder: The labels placeholder.\n",
    "    data_set: The set of images and labels to evaluate, from\n",
    "      input_data.read_data_sets().\n",
    "  \"\"\"\n",
    "  # And run one epoch of eval.\n",
    "  true_count = 0  # Counts the number of correct predictions.\n",
    "  steps_per_epoch = data_set.num_examples // batch_size\n",
    "  num_examples = steps_per_epoch * batch_size\n",
    "  for step in range(steps_per_epoch):\n",
    "    feed_dict = fill_feed_dict(data_set, batch_size,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "  precision = true_count / num_examples\n",
    "  print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-62-a407a310ddda>:28 in <module>.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "WARNING:tensorflow:From d:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\logging_ops.py:264 in merge_all_summaries.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "Step 0: loss = 0.23 (0.047 sec)\n",
      "Step 50: loss = 0.23 (0.016 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 54976  Num correct: 15043  Precision @ 1: 0.2736\n",
      "Validation Data Eval:\n",
      "  Num examples: 4992  Num correct: 1335  Precision @ 1: 0.2674\n",
      "Test Data Eval:\n",
      "  Num examples: 9984  Num correct: 2815  Precision @ 1: 0.2820\n",
      "Step 100: loss = 0.22 (0.016 sec)\n",
      "Step 150: loss = 0.20 (0.016 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 54976  Num correct: 32525  Precision @ 1: 0.5916\n",
      "Validation Data Eval:\n",
      "  Num examples: 4992  Num correct: 2974  Precision @ 1: 0.5958\n",
      "Test Data Eval:\n",
      "  Num examples: 9984  Num correct: 5934  Precision @ 1: 0.5944\n",
      "Step 200: loss = 0.20 (0.047 sec)\n",
      "Step 250: loss = 0.19 (0.016 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 54976  Num correct: 39660  Precision @ 1: 0.7214\n",
      "Validation Data Eval:\n",
      "  Num examples: 4992  Num correct: 3633  Precision @ 1: 0.7278\n",
      "Test Data Eval:\n",
      "  Num examples: 9984  Num correct: 7275  Precision @ 1: 0.7287\n",
      "Step 300: loss = 0.16 (0.047 sec)\n",
      "Step 350: loss = 0.13 (0.016 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 54976  Num correct: 41893  Precision @ 1: 0.7620\n",
      "Validation Data Eval:\n",
      "  Num examples: 4992  Num correct: 3843  Precision @ 1: 0.7698\n",
      "Test Data Eval:\n",
      "  Num examples: 9984  Num correct: 7698  Precision @ 1: 0.7710\n",
      "Step 400: loss = 0.11 (0.047 sec)\n",
      "Step 450: loss = 0.10 (0.016 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 54976  Num correct: 43852  Precision @ 1: 0.7977\n",
      "Validation Data Eval:\n",
      "  Num examples: 4992  Num correct: 3992  Precision @ 1: 0.7997\n",
      "Test Data Eval:\n",
      "  Num examples: 9984  Num correct: 7993  Precision @ 1: 0.8006\n",
      "Step 500: loss = 0.09 (0.031 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 54976  Num correct: 43675  Precision @ 1: 0.7944\n",
      "Validation Data Eval:\n",
      "  Num examples: 4992  Num correct: 3984  Precision @ 1: 0.7981\n",
      "Test Data Eval:\n",
      "  Num examples: 9984  Num correct: 7990  Precision @ 1: 0.8003\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "hidden1= 128\n",
    "hidden2 = 64\n",
    "learning_rate = 0.1\n",
    "num_steps = 501\n",
    "with tf.Graph().as_default():\n",
    "    # Generate placeholders for the images and labels.\n",
    "    images_placeholder, labels_placeholder = placeholder_inputs(\n",
    "        batch_size)\n",
    "\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    logits = inference(images_placeholder,\n",
    "                             hidden1,\n",
    "                             hidden2)\n",
    "\n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    #loss = loss(logits, labels_placeholder)\n",
    "    y_ = tf.log(tf.nn.softmax(logits, name='xentropy'))\n",
    "    loss = -tf.reduce_mean(labels_placeholder*y_, name='xentropy_mean')\n",
    "\n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    train_op = training(loss, learning_rate)\n",
    "\n",
    "    # Add the Op to compare the logits to the labels during evaluation.\n",
    "    eval_correct = evaluation(logits, labels_placeholder)\n",
    "\n",
    "    # Build the summary Tensor based on the TF collection of Summaries.\n",
    "    summary = tf.merge_all_summaries()\n",
    "\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    summary_writer = tf.summary.FileWriter('save/', sess.graph)\n",
    "\n",
    "    # And then after everything is built:\n",
    "\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        start_time = time.time()\n",
    "        #data_batch = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = fill_feed_dict(mnist.train, batch_size, \n",
    "                                 images_placeholder,\n",
    "                                 labels_placeholder)\n",
    "        _, loss_value = sess.run([train_op, loss],\n",
    "                               feed_dict=feed_dict)\n",
    "        duration = time.time() - start_time\n",
    "        if step % 50 == 0:\n",
    "            # Print status to stdout.\n",
    "            print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "        \n",
    "        if (step + 1) % 100 == 0 or (step + 1) == num_steps:\n",
    "            checkpoint_file = os.path.join('save/', 'checkpoint')\n",
    "            saver.save(sess, checkpoint_file, global_step=step)\n",
    "            # Evaluate against the training set.\n",
    "            print('Training Data Eval:')\n",
    "            #train_data = (mnist.train.images, mnist.train.labels)\n",
    "            #feed_dict = {images_placeholder:train_data[0], labels_placeholder:train_data[1]}\n",
    "            #accuracy = sess.run(eval_correct, feed_dict=feed_dict)\n",
    "            #print(accuracy)\n",
    "            do_eval(sess, batch_size, \n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                mnist.train)\n",
    "            # Evaluate against the validation set.\n",
    "            print('Validation Data Eval:')\n",
    "            do_eval(sess, batch_size,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                mnist.validation)\n",
    "            # Evaluate against the test set.\n",
    "            print('Test Data Eval:')\n",
    "            do_eval(sess, batch_size,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                mnist.test)\n",
    "            \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##可视化命令python D:\\Anaconda2\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\tensorboard  --logdir=“”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 1001\n",
    "learning_rate = 0.1\n",
    "summaries_dir = 'summary_dir'\n",
    "fake_data = True\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can't initialize these variables to 0 - the network will get stuck.\n",
    "def weight_variable(shape):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "      mean = tf.reduce_mean(var)\n",
    "      tf.summary.scalar('mean/' + name, mean)\n",
    "      with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "      tf.summary.scalar('stddev/' + name, stddev)\n",
    "      tf.summary.scalar('max/' + name, tf.reduce_max(var))\n",
    "      tf.summary.scalar('min/' + name, tf.reduce_min(var))\n",
    "      tf.summary.histogram(name, var)\n",
    "        \n",
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer.\n",
    "\n",
    "    It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "    It also sets up name scoping so that the resultant graph is easy to read,\n",
    "    and adds a number of summary ops.\n",
    "    \"\"\"\n",
    "    # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "    with tf.name_scope(layer_name):\n",
    "      # This Variable will hold the state of the weights for the layer\n",
    "      with tf.name_scope('weights'):\n",
    "        weights = weight_variable([input_dim, output_dim])\n",
    "        variable_summaries(weights, layer_name + '/weights')\n",
    "      with tf.name_scope('biases'):\n",
    "        biases = bias_variable([output_dim])\n",
    "        variable_summaries(biases, layer_name + '/biases')\n",
    "      with tf.name_scope('Wx_plus_b'):\n",
    "        preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "        tf.summary.histogram(layer_name + '/pre_activations', preactivate)\n",
    "      activations = act(preactivate, name='activation')\n",
    "      tf.summary.histogram(layer_name + '/activations', activations)\n",
    "      return activations\n",
    "\n",
    "def feed_dict(train):\n",
    "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "    if train:\n",
    "      xs, ys = mnist.train.next_batch(100)\n",
    "      k = 0.8\n",
    "    else:\n",
    "      xs, ys = mnist.test.images, mnist.test.labels\n",
    "      k = 1.0\n",
    "    return {x: xs, y_: ys, keep_prob: k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs, ys = mnist.train.next_batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at step 0: 0.082\n",
      "Accuracy at step 50: 0.7661\n",
      "Adding run metadata for 99\n",
      "Accuracy at step 100: 0.8354\n",
      "Accuracy at step 150: 0.8249\n",
      "Adding run metadata for 199\n",
      "Accuracy at step 200: 0.8398\n",
      "Accuracy at step 250: 0.8669\n",
      "Adding run metadata for 299\n",
      "Accuracy at step 300: 0.8757\n",
      "Accuracy at step 350: 0.8701\n",
      "Adding run metadata for 399\n",
      "Accuracy at step 400: 0.8377\n",
      "Accuracy at step 450: 0.8518\n",
      "Adding run metadata for 499\n",
      "Accuracy at step 500: 0.822\n",
      "Accuracy at step 550: 0.862\n",
      "Adding run metadata for 599\n",
      "Accuracy at step 600: 0.8533\n",
      "Accuracy at step 650: 0.8734\n",
      "Adding run metadata for 699\n",
      "Accuracy at step 700: 0.8565\n",
      "Accuracy at step 750: 0.8352\n",
      "Adding run metadata for 799\n",
      "Accuracy at step 800: 0.8281\n",
      "Accuracy at step 850: 0.7548\n",
      "Adding run metadata for 899\n",
      "Accuracy at step 900: 0.8296\n",
      "Accuracy at step 950: 0.7454\n",
      "Adding run metadata for 999\n",
      "Accuracy at step 1000: 0.808\n"
     ]
    }
   ],
   "source": [
    "graph2 = tf.Graph()\n",
    "with graph2.as_default():\n",
    "    sess = tf.InteractiveSession(graph=graph2)\n",
    "    # Create a multilayer model.\n",
    "    # Input placeholders\n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10], name='y-input')\n",
    "    #x = tf.placeholder(tf.int32, [None, 784])\n",
    "    #y_ = tf.placeholder(tf.int32, [None, 10])\n",
    "\n",
    "    with tf.name_scope('input_reshape'):\n",
    "        image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        tf.summary.image('input', image_shaped_input, 10)\n",
    "\n",
    "    hidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "\n",
    "    with tf.name_scope('dropout'):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "        dropped = tf.nn.dropout(hidden1, keep_prob)\n",
    "        # Do not apply softmax activation yet, see below.\n",
    "        y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            # The raw formulation of cross-entropy,\n",
    "            ## tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n",
    "            #                               reduction_indices=[1]))\n",
    "            #\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # So here we use tf.nn.softmax_cross_entropy_with_logits on the\n",
    "            # raw outputs of the nn_layer above, and then average across\n",
    "            # the batch.\n",
    "            diff = tf.nn.softmax_cross_entropy_with_logits(y, y_)\n",
    "            with tf.name_scope('total'):\n",
    "                cross_entropy = tf.reduce_mean(diff)\n",
    "                tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(summaries_dir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(summaries_dir + '/test')\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # Train the model, and also write summaries.\n",
    "    # Every 10th step, measure test-set accuracy, and write test summaries\n",
    "    # All other steps, run train_step on training data, & add training summaries\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        if i % 50 == 0:  # Record summaries and test-set accuracy\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "      \n",
    "            #test_writer.add_summary(summary, i)\n",
    "            print('Accuracy at step %s: %s' % (i, acc))\n",
    "        else:  # Record train set summaries, and train\n",
    "            if i % 100 == 99:  # Record execution stats\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run([merged, train_step],\n",
    "                              feed_dict=feed_dict(True),\n",
    "                              options=run_options,\n",
    "                              run_metadata=run_metadata)\n",
    "                train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "                train_writer.add_summary(summary, i)\n",
    "                print('Adding run metadata for', i)\n",
    "            else:  # Record a summary\n",
    "                summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "                train_writer.add_summary(summary, i)\n",
    "    train_writer.close()\n",
    "    test_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs, ys = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络\n",
    "\n",
    "这里使用LSTM对数字进行识别，具体原理可 参考这篇[文章](http://arxiv.org/pdf/1402.1128v1.pdf)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#手写数字分为10各类，即0-9\n",
    "NUM_CLASSES = 10\n",
    "#图片像素28*28\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "#RNN内部神经元节点数目\n",
    "num_nodes = 64\n",
    "#训练样本群规模\n",
    "batch_size = 128\n",
    "#测试样本群\n",
    "test_size = len(mnist.test.images)\n",
    "#参数初始化\n",
    "initial = 0.01\n",
    "#构建图\n",
    "graph3 = tf.Graph()\n",
    "with graph3.as_default():\n",
    "  \n",
    "  # Parameters(weights):\n",
    "  # 输入门: input, previous output, and bias.\n",
    "  #weights for the input data x(t)\n",
    "  ix = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, num_nodes], -initial, initial))\n",
    "  #weights for the last output h(t-1)\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -initial, initial))\n",
    "  #biase\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))#bias\n",
    "  # 遗忘门: input, previous output, and bias.\n",
    "  #weights for the input i(t)\n",
    "  fx = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, num_nodes], -initial, initial))\n",
    "  #weights for last output h(t-1)\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -initial, initial))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # 记忆细胞: input, state and bias.    \n",
    "  #weights for the input data x(t)\n",
    "  cx = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, num_nodes], -initial, initial))\n",
    "  #weights for the last output h(t-1)\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -initial, initial))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # 输出门: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, num_nodes], -initial, initial))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -initial, initial))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases, multiply the output of LSTM cell and get the\n",
    "  # predictionss\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, NUM_CLASSES], -initial, initial))\n",
    "  b = tf.Variable(tf.zeros([NUM_CLASSES]))\n",
    "  \n",
    "  # LSTM细胞内部计算定义\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. \n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #输入门\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)#i(t)\n",
    "    #遗忘门\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)#f(t)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    #候任状态\n",
    "    candidate_state = tf.tanh(update) #C~(t)   \n",
    "    #最终状态\n",
    "    state = forget_gate * state + input_gate * candidate_state#C(t)\n",
    "    #输出门\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)#O(t)\n",
    "    #输出结果\n",
    "    output = output_gate * tf.tanh(state)#h(t)\n",
    "    return output, state\n",
    "\n",
    "  #images_placeholder, labels_placeholder = placeholder_inputs(batch_size)\n",
    "  images_placeholder = tf.placeholder(tf.float32, shape=(None, IMAGE_PIXELS))\n",
    "  labels_placeholder = tf.placeholder(tf.float32, shape=(None, NUM_CLASSES))\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  output, state = lstm_cell(images_placeholder, output, state)\n",
    "\n",
    "  # 将当前状态和输出值记录下来\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    #预测输出结果，非归一化\n",
    "    logits = tf.nn.xw_plus_b(output, w, b)#\n",
    "    #损失函数\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\n",
    "                              (logits, labels_placeholder))\n",
    "  optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "  \n",
    "  #计算测试结果\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[test_size, IMAGE_PIXELS])\n",
    "  sample_label = tf.placeholder(tf.float32, shape=[test_size, NUM_CLASSES])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([test_size, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([test_size, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([test_size, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([test_size, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "      #计算准确度\n",
    "    correct_prediction = tf.equal(tf.argmax(sample_prediction,1), tf.argmax(sample_label,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.30255\n",
      "Accuracy: 0.1121\n",
      "Loss: 1.14141\n",
      "Accuracy: 0.7773\n",
      "Loss: 0.620363\n",
      "Accuracy: 0.8639\n",
      "Loss: 0.425818\n",
      "Accuracy: 0.8871\n",
      "Loss: 0.409023\n",
      "Accuracy: 0.8979\n",
      "Loss: 0.258485\n",
      "Accuracy: 0.9067\n",
      "Loss: 0.359537\n",
      "Accuracy: 0.9094\n",
      "Loss: 0.338571\n",
      "Accuracy: 0.9134\n",
      "Loss: 0.308494\n",
      "Accuracy: 0.9153\n",
      "Loss: 0.327692\n",
      "Accuracy: 0.9183\n",
      "Loss: 0.341893\n",
      "Accuracy: 0.9178\n",
      "Loss: 0.292808\n",
      "Accuracy: 0.9233\n",
      "Loss: 0.224088\n",
      "Accuracy: 0.9249\n",
      "Loss: 0.338388\n",
      "Accuracy: 0.9268\n",
      "Loss: 0.106679\n",
      "Accuracy: 0.929\n",
      "Loss: 0.170672\n",
      "Accuracy: 0.929\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "with tf.Session(graph=graph3) as sess:\n",
    "    #初始化所有变量\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for i in range(num_steps):\n",
    "        #获取组训练数据\n",
    "        images, labels = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {images_placeholder: images, labels_placeholder: labels}\n",
    "        #进行训练学习\n",
    "        _, ls = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        if i%200 == 0:\n",
    "            print('Training Loss:', ls)\n",
    "            feed_dict = {sample_input: mnist.test.images, sample_label: mnist.test.labels}\n",
    "            acc = sess.run(accuracy, feed_dict=feed_dict)\n",
    "            print('Testing Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 循环神经网络2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们直接用tensorflow自带的循环神经网络模块对手写数字进行分类。具体可以参考这篇[博客](http://blog.topspeedsnail.com/archives/10443)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#神经网络内部神经元数量\n",
    "num_nodes = 64\n",
    "#手写数字分为10各类，即0-9\n",
    "NUM_CLASSES = 10\n",
    "#图片像素28*28\n",
    "IMAGE_SIZE = 28\n",
    "TIME_STEPS = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "def recurrent_neural_network(data):\n",
    "    layer = {'w_':tf.Variable(tf.random_normal([num_nodes, NUM_CLASSES])),\n",
    "             'b_':tf.Variable(tf.random_normal([NUM_CLASSES]))}\n",
    "    #LSTM计算单元设置\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_nodes)\n",
    "    #输入数据尺寸变换\n",
    "    #Initial Data: batch_size,image_size,image_size\n",
    "    data = tf.transpose(data, [1,0,2])\n",
    "    #Then: image_size,batch_size,image_size\n",
    "    data = tf.reshape(data, [-1, IMAGE_SIZE])\n",
    "    #Now:image_size*batch_size,TIME_STEPS\n",
    "    data = tf.split(0, TIME_STEPS, data)\n",
    "    outputs, status = tf.nn.rnn(lstm_cell, data, dtype=tf.float32)\n",
    " \n",
    "    output = tf.add(tf.matmul(outputs[-1], layer['w_']), layer['b_'])\n",
    " \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1019\n",
      "Accuracy: 0.6228\n",
      "Accuracy: 0.7867\n",
      "Accuracy: 0.8611\n",
      "Accuracy: 0.8845\n",
      "Accuracy: 0.8851\n",
      "Accuracy: 0.9154\n",
      "Accuracy: 0.8998\n",
      "Accuracy: 0.9249\n",
      "Accuracy: 0.9368\n",
      "Accuracy: 0.9336\n",
      "Accuracy: 0.9407\n",
      "Accuracy: 0.9407\n",
      "Accuracy: 0.9481\n",
      "Accuracy: 0.9484\n",
      "Accuracy: 0.9572\n",
      "Accuracy: 0.9574\n",
      "Accuracy: 0.9609\n",
      "Accuracy: 0.9596\n",
      "Accuracy: 0.9581\n",
      "Accuracy: 0.9612\n"
     ]
    }
   ],
   "source": [
    "#构建一个新的图\n",
    "num_steps = 1001\n",
    "batch_size = 128\n",
    "graph4 = tf.Graph()\n",
    "with graph4.as_default():\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE))\n",
    "    labels_placeholder = tf.placeholder(tf.float32, shape=(None, NUM_CLASSES))   \n",
    "    logits = recurrent_neural_network(images_placeholder)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits, labels_placeholder)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    #计算准确度\n",
    "    predict = tf.nn.softmax(logits)\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), tf.argmax(labels_placeholder,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    #创建会话进行运算\n",
    "    sess = tf.Session(graph=graph4)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for i in range(num_steps):\n",
    "        x, y = mnist.train.next_batch(batch_size)\n",
    "        x = x.reshape([batch_size, IMAGE_SIZE, IMAGE_SIZE])\n",
    "        feed_dict = {images_placeholder: x, labels_placeholder:y}\n",
    "        _, l = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        if i %50 == 0:\n",
    "            x, y = mnist.test.images, mnist.test.labels\n",
    "            x = x.reshape([len(x), IMAGE_SIZE, IMAGE_SIZE])\n",
    "            feed_dict = {images_placeholder: x, labels_placeholder:y}\n",
    "            acc = sess.run(accuracy, feed_dict=feed_dict)\n",
    "            print('Accuracy:', acc)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
