{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用 NumPy 生成假数据(phony data), 总共 100 个点.\n",
    "x_data = np.float32(np.random.rand(2, 100)) # 随机输入\n",
    "y_data = np.dot([0.100, 0.200], x_data) + 0.300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造一个线性模型\n",
    "# \n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))\n",
    "y = tf.matmul(W, x_data) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[-0.5526787,  1.027071 ]], dtype=float32), array([ 0.42350525], dtype=float32)]\n",
      "20 [array([[-0.03690662,  0.3674413 ]], dtype=float32), array([ 0.28259146], dtype=float32)]\n",
      "40 [array([[ 0.07512739,  0.23839875]], dtype=float32), array([ 0.29263803], dtype=float32)]\n",
      "60 [array([[ 0.09576317,  0.20909017]], dtype=float32), array([ 0.29740426], dtype=float32)]\n",
      "80 [array([[ 0.09936837,  0.2022239 ]], dtype=float32), array([ 0.2991558], dtype=float32)]\n",
      "100 [array([[ 0.09993653,  0.20056173]], dtype=float32), array([ 0.29973716], dtype=float32)]\n",
      "120 [array([[ 0.10000557,  0.20014605]], dtype=float32), array([ 0.29992023], dtype=float32)]\n",
      "140 [array([[ 0.10000642,  0.20003897]], dtype=float32), array([ 0.29997617], dtype=float32)]\n",
      "160 [array([[ 0.10000281,  0.20001058]], dtype=float32), array([ 0.29999298], dtype=float32)]\n",
      "180 [array([[ 0.10000101,  0.20000292]], dtype=float32), array([ 0.29999793], dtype=float32)]\n",
      "200 [array([[ 0.10000034,  0.20000082]], dtype=float32), array([ 0.29999939], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# 最小化方差\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 启动图 (graph)\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "#tf.global_variables_initializer().run(session=sess)\n",
    "# 拟合平面\n",
    "for step in range(0, 201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run([W, b]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常量计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建一个常量 op, 产生一个 1x2 矩阵. 这个 op 被作为一个节点\n",
    "# 加到默认图中.\n",
    "#\n",
    "# 构造器的返回值代表该常量 op 的返回值.\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "\n",
    "# 创建另外一个常量 op, 产生一个 2x1 矩阵.\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "# 创建一个矩阵乘法 matmul op , 把 'matrix1' 和 'matrix2' 作为输入.\n",
    "# 返回值 'product' 代表矩阵乘法的结果.\n",
    "product = tf.matmul(matrix1, matrix2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_2:0\", shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "# 启动默认图.\n",
    "sess = tf.Session()\n",
    "\n",
    "# 调用 sess 的 'run()' 方法来执行矩阵乘法 op, 传入 'product' 作为该方法的参数. \n",
    "# 上面提到, 'product' 代表了矩阵乘法 op 的输出, 传入它是向方法表明, 我们希望取回\n",
    "# 矩阵乘法 op 的输出.\n",
    "#\n",
    "# 整个执行过程是自动化的, 会话负责传递 op 所需的全部输入. op 通常是并发执行的.\n",
    "# \n",
    "# 函数调用 'run(product)' 触发了图中三个 op (两个常量 op 和一个矩阵乘法 op) 的执行.\n",
    "#\n",
    "# 返回值 'result' 是一个 numpy `ndarray` 对象.\n",
    "result = sess.run(product)\n",
    "print(result)\n",
    "# ==> [[ 12.]]\n",
    "\n",
    "# 任务完成, 关闭会话.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  3.]]\n"
     ]
    }
   ],
   "source": [
    "print(matrix1.eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交互式会话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2. -1.]\n"
     ]
    }
   ],
   "source": [
    "# 进入一个交互式 TensorFlow 会话.\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.Variable([1.0, 2.0])\n",
    "a = tf.constant([3.0, 3.0])\n",
    "\n",
    "# 使用初始化器 initializer op 的 run() 方法初始化 'x' \n",
    "x.initializer.run()\n",
    "\n",
    "# 增加一个减法 sub op, 从 'x' 减去 'a'. 运行减法 op, 输出结果 \n",
    "sub = tf.sub(x, a)\n",
    "print(sub.eval())\n",
    "# ==> [-2. -1.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 创建一个变量, 初始化为标量 0.\n",
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "# 创建一个 op, 其作用是使 state 增加 1\n",
    "\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "# 启动图后, 变量必须先经过`初始化` (init) op 初始化,\n",
    "# 首先必须增加一个`初始化` op 到图中.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# 启动图, 运行 op\n",
    "with tf.Session() as sess:\n",
    "  # 运行 'init' op\n",
    "  sess.run(init_op)\n",
    "  # 打印 'state' 的初始值\n",
    "  print(sess.run(state))\n",
    "  # 运行 op, 更新 'state', 并打印 'state'\n",
    "  for _ in range(3):\n",
    "    sess.run(update)\n",
    "    print(sess.run(state))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 14.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output = tf.mul(input1, input2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST入门"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = mnist.train.images\n",
    "train_labels = mnist.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#定义输入数据\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "#期望输出标签\n",
    "y_ = tf.placeholder(tf.float32, [None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义参数变量\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义输出标签值 \n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义目标函数\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义训练方式，梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3045\n",
      "0.8699\n",
      "0.8743\n",
      "0.8549\n",
      "0.9014\n",
      "0.8833\n",
      "0.9072\n",
      "0.9033\n",
      "0.9099\n",
      "0.9107\n",
      "0.9028\n",
      "0.9096\n",
      "0.9097\n",
      "0.8926\n",
      "0.9151\n",
      "0.9106\n",
      "0.9122\n",
      "0.8925\n",
      "0.9165\n",
      "0.9188\n",
      "0.8992\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "batch_size = 64\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.      \n",
    "        batch_data = train_images[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        #Pass the data to the feed dictionary\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "        _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "        if step%50 == 0:\n",
    "            #定义模型评价指标\n",
    "            correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST进阶（卷积）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#变量定义\n",
    "#定义输入数据\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "#期望输出标签\n",
    "y_ = tf.placeholder(tf.float32, [None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第一层卷积\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第二层卷积\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#全连接层\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#丢弃\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#输出层，softmax\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#损失函数\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "#梯度下降法\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "#计算准确度\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.1139\n",
      "Accuracy 0.7469\n",
      "Accuracy 0.8609\n",
      "Accuracy 0.8959\n",
      "Accuracy 0.9129\n",
      "Accuracy 0.9256\n",
      "Accuracy 0.9374\n",
      "Accuracy 0.9373\n",
      "Accuracy 0.945\n",
      "Accuracy 0.9445\n",
      "Accuracy 0.9461\n",
      "Accuracy 0.9526\n",
      "Accuracy 0.9536\n",
      "Accuracy 0.9566\n",
      "Accuracy 0.9582\n",
      "Accuracy 0.9581\n",
      "Accuracy 0.9606\n",
      "Accuracy 0.9592\n",
      "Accuracy 0.9633\n",
      "Accuracy 0.9637\n",
      "Accuracy 0.9651\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "batch_size = 64\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.      \n",
    "        batch_data = train_images[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        #Pass the data to the feed dictionary\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels, keep_prob:0.5}\n",
    "        sess.run(train_step, feed_dict=feed_dict)\n",
    "        if step%50 == 0:\n",
    "            result = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob:1})\n",
    "            print('Accuracy', result)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 命名空间和可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The MNIST dataset has 10 classes, representing the digits 0 through 9.\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# The MNIST images are always 28x28 pixels.\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "\n",
    "def inference(images, hidden1_units, hidden2_units):\n",
    "  \"\"\"Build the MNIST model up to where it may be used for inference.\n",
    "\n",
    "  Args:\n",
    "    images: Images placeholder, from inputs().\n",
    "    hidden1_units: Size of the first hidden layer.\n",
    "    hidden2_units: Size of the second hidden layer.\n",
    "\n",
    "  Returns:\n",
    "    softmax_linear: Output tensor with the computed logits.\n",
    "  \"\"\"\n",
    "  # Hidden 1\n",
    "  with tf.name_scope('hidden1'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([IMAGE_PIXELS, hidden1_units],\n",
    "                            stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                         name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "  # Hidden 2\n",
    "  with tf.name_scope('hidden2'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                         name='biases')\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "  # Linear\n",
    "  with tf.name_scope('softmax_linear'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([hidden2_units, NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([NUM_CLASSES]),\n",
    "                         name='biases')\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "  return logits\n",
    "\n",
    "\n",
    "def loss(logits, labels):\n",
    "  \"\"\"Calculates the loss from the logits and the labels.\n",
    "\n",
    "  Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size].\n",
    "\n",
    "  Returns:\n",
    "    loss: Loss tensor of type float.\n",
    "  \"\"\"\n",
    "  #labels = tf.to_int64(labels)\n",
    "  #cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      #logits, labels, name='xentropy')\n",
    "  #loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "  y_ = tf.nn.softmax(logits, name='xentropy')\n",
    "  loss = -tf.reduce_mean(labels*tf.log(y_), name='xentropy_mean')\n",
    "  return loss\n",
    "\n",
    "\n",
    "def training(loss, learning_rate):\n",
    "  \"\"\"Sets up the training Ops.\n",
    "\n",
    "  Creates a summarizer to track the loss over time in TensorBoard.\n",
    "\n",
    "  Creates an optimizer and applies the gradients to all trainable variables.\n",
    "\n",
    "  The Op returned by this function is what must be passed to the\n",
    "  `sess.run()` call to cause the model to train.\n",
    "\n",
    "  Args:\n",
    "    loss: Loss tensor, from loss().\n",
    "    learning_rate: The learning rate to use for gradient descent.\n",
    "\n",
    "  Returns:\n",
    "    train_op: The Op for training.\n",
    "  \"\"\"\n",
    "  # Add a scalar summary for the snapshot loss.\n",
    "  tf.summary.scalar(loss.op.name, loss)\n",
    "  # Create the gradient descent optimizer with the given learning rate.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  # Create a variable to track the global step.\n",
    "  global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "  # Use the optimizer to apply the gradients that minimize the loss\n",
    "  # (and also increment the global step counter) as a single training step.\n",
    "  train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "  return train_op\n",
    "\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "  \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "\n",
    "  Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size], with values in the\n",
    "      range [0, NUM_CLASSES).\n",
    "\n",
    "  Returns:\n",
    "    A scalar int32 tensor with the number of examples (out of batch_size)\n",
    "    that were predicted correctly.\n",
    "  \"\"\"\n",
    "    #计算准确度\n",
    "  y_hat = tf.nn.softmax(logits, name='xentropy')\n",
    "  correct_prediction = tf.equal(tf.argmax(y_hat,1), tf.argmax(labels,1))\n",
    "  #accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "  #return accuracy\n",
    "\n",
    "  # For a classifier model, we can use the in_top_k Op.\n",
    "  # It returns a bool tensor with shape [batch_size] that is true for\n",
    "  # the examples where the label is in the top k (here k=1)\n",
    "  # of all logits for that example.\n",
    "\n",
    "  #correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "  # Return the number of true entries.\n",
    "  return tf.reduce_sum(tf.cast(correct_prediction, tf.int32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size):\n",
    "  \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "\n",
    "  These placeholders are used as inputs by the rest of the model building\n",
    "  code and will be fed from the downloaded data in the .run() loop, below.\n",
    "\n",
    "  Args:\n",
    "    batch_size: The batch size will be baked into both placeholders.\n",
    "\n",
    "  Returns:\n",
    "    images_placeholder: Images placeholder.\n",
    "    labels_placeholder: Labels placeholder.\n",
    "  \"\"\"\n",
    "  # Note that the shapes of the placeholders match the shapes of the full\n",
    "  # image and label tensors, except the first dimension is now batch_size\n",
    "  # rather than the full size of the train or test data sets.\n",
    "  images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                         IMAGE_PIXELS))\n",
    "  labels_placeholder = tf.placeholder(tf.float32, shape=(batch_size, NUM_CLASSES))\n",
    "  return images_placeholder, labels_placeholder\n",
    "\n",
    "\n",
    "def fill_feed_dict(data_set, batch_size, images_pl, labels_pl):\n",
    "  \"\"\"Fills the feed_dict for training the given step.\n",
    "\n",
    "  A feed_dict takes the form of:\n",
    "  feed_dict = {\n",
    "      <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "      ....\n",
    "  }\n",
    "\n",
    "  Args:\n",
    "    data_set: The set of images and labels, from input_data.read_data_sets()\n",
    "    images_pl: The images placeholder, from placeholder_inputs().\n",
    "    labels_pl: The labels placeholder, from placeholder_inputs().\n",
    "\n",
    "  Returns:\n",
    "    feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "  \"\"\"\n",
    "  # Create the feed_dict for the placeholders filled with the next\n",
    "  # `batch size` examples.\n",
    "  images_feed, labels_feed = data_set.next_batch(batch_size)\n",
    "  feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "  }\n",
    "  return feed_dict\n",
    "\n",
    "\n",
    "def do_eval(sess, batch_size,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "  \"\"\"Runs one evaluation against the full epoch of data.\n",
    "\n",
    "  Args:\n",
    "    sess: The session in which the model has been trained.\n",
    "    eval_correct: The Tensor that returns the number of correct predictions.\n",
    "    images_placeholder: The images placeholder.\n",
    "    labels_placeholder: The labels placeholder.\n",
    "    data_set: The set of images and labels to evaluate, from\n",
    "      input_data.read_data_sets().\n",
    "  \"\"\"\n",
    "  # And run one epoch of eval.\n",
    "  true_count = 0  # Counts the number of correct predictions.\n",
    "  steps_per_epoch = data_set.num_examples // batch_size\n",
    "  num_examples = steps_per_epoch * batch_size\n",
    "  for step in range(steps_per_epoch):\n",
    "    feed_dict = fill_feed_dict(data_set, batch_size,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "  precision = true_count / num_examples\n",
    "  print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-62-a407a310ddda>:28 in <module>.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "WARNING:tensorflow:From d:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\logging_ops.py:264 in merge_all_summaries.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "Step 0: loss = 0.23 (0.047 sec)\n",
      "Step 50: loss = 0.23 (0.016 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 54976  Num correct: 15043  Precision @ 1: 0.2736\n",
      "Validation Data Eval:\n",
      "  Num examples: 4992  Num correct: 1335  Precision @ 1: 0.2674\n",
      "Test Data Eval:\n",
      "  Num examples: 9984  Num correct: 2815  Precision @ 1: 0.2820\n",
      "Step 100: loss = 0.22 (0.016 sec)\n",
      "Step 150: loss = 0.20 (0.016 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 54976  Num correct: 32525  Precision @ 1: 0.5916\n",
      "Validation Data Eval:\n",
      "  Num examples: 4992  Num correct: 2974  Precision @ 1: 0.5958\n",
      "Test Data Eval:\n",
      "  Num examples: 9984  Num correct: 5934  Precision @ 1: 0.5944\n",
      "Step 200: loss = 0.20 (0.047 sec)\n",
      "Step 250: loss = 0.19 (0.016 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 54976  Num correct: 39660  Precision @ 1: 0.7214\n",
      "Validation Data Eval:\n",
      "  Num examples: 4992  Num correct: 3633  Precision @ 1: 0.7278\n",
      "Test Data Eval:\n",
      "  Num examples: 9984  Num correct: 7275  Precision @ 1: 0.7287\n",
      "Step 300: loss = 0.16 (0.047 sec)\n",
      "Step 350: loss = 0.13 (0.016 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 54976  Num correct: 41893  Precision @ 1: 0.7620\n",
      "Validation Data Eval:\n",
      "  Num examples: 4992  Num correct: 3843  Precision @ 1: 0.7698\n",
      "Test Data Eval:\n",
      "  Num examples: 9984  Num correct: 7698  Precision @ 1: 0.7710\n",
      "Step 400: loss = 0.11 (0.047 sec)\n",
      "Step 450: loss = 0.10 (0.016 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 54976  Num correct: 43852  Precision @ 1: 0.7977\n",
      "Validation Data Eval:\n",
      "  Num examples: 4992  Num correct: 3992  Precision @ 1: 0.7997\n",
      "Test Data Eval:\n",
      "  Num examples: 9984  Num correct: 7993  Precision @ 1: 0.8006\n",
      "Step 500: loss = 0.09 (0.031 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 54976  Num correct: 43675  Precision @ 1: 0.7944\n",
      "Validation Data Eval:\n",
      "  Num examples: 4992  Num correct: 3984  Precision @ 1: 0.7981\n",
      "Test Data Eval:\n",
      "  Num examples: 9984  Num correct: 7990  Precision @ 1: 0.8003\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "hidden1= 128\n",
    "hidden2 = 64\n",
    "learning_rate = 0.1\n",
    "num_steps = 501\n",
    "with tf.Graph().as_default():\n",
    "    # Generate placeholders for the images and labels.\n",
    "    images_placeholder, labels_placeholder = placeholder_inputs(\n",
    "        batch_size)\n",
    "\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    logits = inference(images_placeholder,\n",
    "                             hidden1,\n",
    "                             hidden2)\n",
    "\n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    #loss = loss(logits, labels_placeholder)\n",
    "    y_ = tf.log(tf.nn.softmax(logits, name='xentropy'))\n",
    "    loss = -tf.reduce_mean(labels_placeholder*y_, name='xentropy_mean')\n",
    "\n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    train_op = training(loss, learning_rate)\n",
    "\n",
    "    # Add the Op to compare the logits to the labels during evaluation.\n",
    "    eval_correct = evaluation(logits, labels_placeholder)\n",
    "\n",
    "    # Build the summary Tensor based on the TF collection of Summaries.\n",
    "    summary = tf.merge_all_summaries()\n",
    "\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    summary_writer = tf.summary.FileWriter('save/', sess.graph)\n",
    "\n",
    "    # And then after everything is built:\n",
    "\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        start_time = time.time()\n",
    "        #data_batch = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = fill_feed_dict(mnist.train, batch_size, \n",
    "                                 images_placeholder,\n",
    "                                 labels_placeholder)\n",
    "        _, loss_value = sess.run([train_op, loss],\n",
    "                               feed_dict=feed_dict)\n",
    "        duration = time.time() - start_time\n",
    "        if step % 50 == 0:\n",
    "            # Print status to stdout.\n",
    "            print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "        \n",
    "        if (step + 1) % 100 == 0 or (step + 1) == num_steps:\n",
    "            checkpoint_file = os.path.join('save/', 'checkpoint')\n",
    "            saver.save(sess, checkpoint_file, global_step=step)\n",
    "            # Evaluate against the training set.\n",
    "            print('Training Data Eval:')\n",
    "            #train_data = (mnist.train.images, mnist.train.labels)\n",
    "            #feed_dict = {images_placeholder:train_data[0], labels_placeholder:train_data[1]}\n",
    "            #accuracy = sess.run(eval_correct, feed_dict=feed_dict)\n",
    "            #print(accuracy)\n",
    "            do_eval(sess, batch_size, \n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                mnist.train)\n",
    "            # Evaluate against the validation set.\n",
    "            print('Validation Data Eval:')\n",
    "            do_eval(sess, batch_size,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                mnist.validation)\n",
    "            # Evaluate against the test set.\n",
    "            print('Test Data Eval:')\n",
    "            do_eval(sess, batch_size,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                mnist.test)\n",
    "            \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
